{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:40px 0;\"> \n",
    "    <h1 style=\"color: white;\"> <b>Intelligent Search Demo</b> </h1>.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"alice_in_wonderland.jpg\"\n",
    "     alt=\"Alice in wonderland\"\n",
    "     style=\"float: left; margin-right: 10px; max-width:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Alice's Adventures in Wonderland (commonly shortened to Alice in Wonderland) is an 1865 novel written by English author Charles Lutwidge Dodgson under the pseudonym Lewis Carroll. It tells of a young girl named Alice falling through a rabbit hole into a fantasy world populated by peculiar, anthropomorphic creatures. The tale plays with logic, giving the story lasting popularity with adults as well as with children. It is considered to be one of the best examples of the literary nonsense genre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo demonstrates the use of Intelligent Search capability on the text of \"Alice in Wonderland\". The demo notebook is structures as follows:\n",
    "- First we get the text of the book from a nltk corpus and process it for consumption by a ranker\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:10px 0;\"> \n",
    "    <h2 style=\"color: white;\"> <b>Question</b> </h2>.\n",
    " </div>\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = 'What is the best way to explain?'\n",
    "# question = 'Why should we not be going back to yesterday?'\n",
    "question = 'What did the rabbit took out of its waistcoat-pocket?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:10px 0;\"> \n",
    "    <h2 style=\"color: white;\"> <b>Get the book text</b> </h2>.\n",
    " </div>\n",
    "<br/>\n",
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive which we are going to use in this exercse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.core.display import display, HTML\n",
    "from deeppavlov import build_model, configs\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name = 'carroll-alice.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2> Number of paragraphs: 817</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h2> Number of paragraphs: ' + str(len(gutenberg.paras(book_name))) + '</h2>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2> Number of sentences: 1703</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h2> Number of sentences: ' + str(len(gutenberg.sents(book_name))) + '</h2>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2> Number of words: 34110</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h2> Number of words: ' + str(len(gutenberg.words(book_name))) + '</h2>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['There',\n",
       "  'was',\n",
       "  'nothing',\n",
       "  'so',\n",
       "  'VERY',\n",
       "  'remarkable',\n",
       "  'in',\n",
       "  'that',\n",
       "  ';',\n",
       "  'nor',\n",
       "  'did',\n",
       "  'Alice',\n",
       "  'think',\n",
       "  'it',\n",
       "  'so',\n",
       "  'VERY',\n",
       "  'much',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'way',\n",
       "  'to',\n",
       "  'hear',\n",
       "  'the',\n",
       "  'Rabbit',\n",
       "  'say',\n",
       "  'to',\n",
       "  'itself',\n",
       "  ',',\n",
       "  \"'\",\n",
       "  'Oh',\n",
       "  'dear',\n",
       "  '!'],\n",
       " ['Oh', 'dear', '!'],\n",
       " ['I', 'shall', 'be', 'late', \"!'\"],\n",
       " ['(',\n",
       "  'when',\n",
       "  'she',\n",
       "  'thought',\n",
       "  'it',\n",
       "  'over',\n",
       "  'afterwards',\n",
       "  ',',\n",
       "  'it',\n",
       "  'occurred',\n",
       "  'to',\n",
       "  'her',\n",
       "  'that',\n",
       "  'she',\n",
       "  'ought',\n",
       "  'to',\n",
       "  'have',\n",
       "  'wondered',\n",
       "  'at',\n",
       "  'this',\n",
       "  ',',\n",
       "  'but',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'it',\n",
       "  'all',\n",
       "  'seemed',\n",
       "  'quite',\n",
       "  'natural',\n",
       "  ');',\n",
       "  'but',\n",
       "  'when',\n",
       "  'the',\n",
       "  'Rabbit',\n",
       "  'actually',\n",
       "  'TOOK',\n",
       "  'A',\n",
       "  'WATCH',\n",
       "  'OUT',\n",
       "  'OF',\n",
       "  'ITS',\n",
       "  'WAISTCOAT',\n",
       "  '-',\n",
       "  'POCKET',\n",
       "  ',',\n",
       "  'and',\n",
       "  'looked',\n",
       "  'at',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'then',\n",
       "  'hurried',\n",
       "  'on',\n",
       "  ',',\n",
       "  'Alice',\n",
       "  'started',\n",
       "  'to',\n",
       "  'her',\n",
       "  'feet',\n",
       "  ',',\n",
       "  'for',\n",
       "  'it',\n",
       "  'flashed',\n",
       "  'across',\n",
       "  'her',\n",
       "  'mind',\n",
       "  'that',\n",
       "  'she',\n",
       "  'had',\n",
       "  'never',\n",
       "  'before',\n",
       "  'seen',\n",
       "  'a',\n",
       "  'rabbit',\n",
       "  'with',\n",
       "  'either',\n",
       "  'a',\n",
       "  'waistcoat',\n",
       "  '-',\n",
       "  'pocket',\n",
       "  ',',\n",
       "  'or',\n",
       "  'a',\n",
       "  'watch',\n",
       "  'to',\n",
       "  'take',\n",
       "  'out',\n",
       "  'of',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'burning',\n",
       "  'with',\n",
       "  'curiosity',\n",
       "  ',',\n",
       "  'she',\n",
       "  'ran',\n",
       "  'across',\n",
       "  'the',\n",
       "  'field',\n",
       "  'after',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'fortunately',\n",
       "  'was',\n",
       "  'just',\n",
       "  'in',\n",
       "  'time',\n",
       "  'to',\n",
       "  'see',\n",
       "  'it',\n",
       "  'pop',\n",
       "  'down',\n",
       "  'a',\n",
       "  'large',\n",
       "  'rabbit',\n",
       "  '-',\n",
       "  'hole',\n",
       "  'under',\n",
       "  'the',\n",
       "  'hedge',\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.paras(book_name)[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:10px 0;\"> \n",
    "    <h2 style=\"color: white;\"> <b>Pre-process the text</b> </h2>.\n",
    " </div>\n",
    "<br/>\n",
    "\n",
    "In this nltk corpus each sentence is represented as a list of words in the sentence, and each paragraph is represented as a list of sentences. In the preporcessing step we detokenize the words in the sentence to reconstruct the sentence and then merge the sentences to reconstruct the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_list = []\n",
    "for para in gutenberg.paras(book_name):\n",
    "    sentence_list = []\n",
    "    for sentence in para:\n",
    "        sentence_list.append(TreebankWordDetokenizer().detokenize(sentence))\n",
    "    para_list.append(\" \".join([sent for sent in sentence_list]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself ,' Oh dear! Oh dear! I shall be late!' ( when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural ); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT - POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat - pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit - hole under the hedge.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_list[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:10px 0;\"> \n",
    "    <h2 style=\"color: white;\"> <b>Part 1: Implement the ranker</b> </h2>.\n",
    " </div>\n",
    "<br/>\n",
    "Ranker goes through the corpus of documents to identify the relevant passages that may potentially contain the answer to the query. In this demo we have used a simple tf-idf based ranker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the book text and question into count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_total_text = count_vectorizer.fit_transform(para_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_question = count_vectorizer.transform([question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817, 10658)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(count_vec_total_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10658)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(count_vec_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the cosine similarity of the question vector from the paragraph vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_array = cosine_similarity(count_vec_question, count_vec_total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_array = distance_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "817"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distance_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.18257419, 0.        , 0.06085806, 0.42802583,\n",
       "       0.        , 0.07106691, 0.0804518 , 0.        , 0.        ,\n",
       "       0.        , 0.0255655 , 0.0789337 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0758098 , 0.        , 0.        ,\n",
       "       0.09774528, 0.04188539, 0.02879561, 0.        , 0.        ,\n",
       "       0.        , 0.04778185, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.03178209, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.05227084, 0.        , 0.        ,\n",
       "       0.10540926, 0.        , 0.07332356, 0.        , 0.        ,\n",
       "       0.08908708, 0.05832118, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.06711561, 0.        , 0.09901475,\n",
       "       0.        , 0.        , 0.        , 0.14213381, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.05143445, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.13608276, 0.03910309,\n",
       "       0.05407381, 0.09072184, 0.04233338, 0.03462717, 0.05504819,\n",
       "       0.        , 0.        , 0.04188539, 0.        , 0.        ,\n",
       "       0.        , 0.09829464, 0.05954913, 0.10127394, 0.0758098 ,\n",
       "       0.09365858, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03874921, 0.        , 0.        ,\n",
       "       0.06375767, 0.        , 0.15430335, 0.08908708, 0.06900656,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.02938635, 0.        , 0.        , 0.05407381,\n",
       "       0.        , 0.        , 0.07856742, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.06900656, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07856742, 0.0758098 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05227084,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05407381,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03487901,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.08164966,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.05954913, 0.        , 0.        , 0.07856742,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.13443321,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.06711561, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04845016, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.06711561, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.04536092, 0.12309149, 0.        , 0.        ,\n",
       "       0.        , 0.04327423, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.07332356, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.06900656, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19802951, 0.        ,\n",
       "       0.        , 0.        , 0.08908708, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.14664712, 0.12309149,\n",
       "       0.        , 0.        , 0.        , 0.1132277 , 0.        ,\n",
       "       0.        , 0.04428074, 0.16329932, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.05407381, 0.        , 0.        , 0.05832118,\n",
       "       0.        , 0.        , 0.04376881, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.06475239,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.15430335, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.08908708, 0.        , 0.06537205,\n",
       "       0.        , 0.27216553, 0.        , 0.11433239, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.0531494 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.11215443, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07856742, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.13608276, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.04188539, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13608276, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.15430335, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09365858, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08908708, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17817416,\n",
       "       0.09901475, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.12309149, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04481107, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.08908708, 0.        , 0.1028689 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.03946685, 0.        , 0.14664712, 0.        , 0.        ,\n",
       "       0.        , 0.06375767, 0.        , 0.05063697, 0.        ,\n",
       "       0.09365858, 0.07856742, 0.        , 0.12309149, 0.08908708,\n",
       "       0.        , 0.12309149, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07106691, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.18257419, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.13608276, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08512565, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.11215443, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05954913, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08512565, 0.        ,\n",
       "       0.09901475, 0.        , 0.        , 0.07106691, 0.        ,\n",
       "       0.10540926, 0.        , 0.09901475, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.1132277 , 0.        ,\n",
       "       0.15430335, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08908708, 0.06900656, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03910309,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the top n similar passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_passages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4   There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself ,' Oh dear! Oh dear! I shall be late!' ( when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural ); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT - POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat - pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit - hole under the hedge.\n",
      "--------------------\n",
      "456   ' Did you say \" What a pity!\"?' the Rabbit asked.\n",
      "--------------------\n",
      "383   ' What did they live on?' said Alice, who always took a great interest in questions of eating and drinking.\n",
      "--------------------\n",
      "712   ' You did!' said the Hatter.\n",
      "--------------------\n",
      "1   CHAPTER I. Down the Rabbit - Hole\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for index in distance_array.argsort()[::-1][:top_n]:\n",
    "    print(index, ' ', para_list[index])\n",
    "    print('-' * 20)\n",
    "    top_passages.append(para_list[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:10px 0;\"> \n",
    "    <h2 style=\"color: white;\"> <b>Part 2 : Use question answer model to extract the answer </b> </h2>.\n",
    " </div>\n",
    "<br/>\n",
    "\n",
    "In this part we demonstrate the use of two off the shelf question answer models trained on open source SQUAD dataset. These question answer models use the power of transfer learning via word embeddings to enhance the ability of the question answer models across different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a : Use DeepPavlov API to extract the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-07 14:26:19.11 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip download because of matching hashes\n",
      "2021-01-07 14:26:20.340 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_bert.tar.gz download because of matching hashes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:81: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:178: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:154: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:166: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:89: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/models/bert/bert_squad.py:94: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-07 14:27:19.861 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /Users/asingh/.deeppavlov/models/squad_bert/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/myenv/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/asingh/.deeppavlov/models/squad_bert/model\n"
     ]
    }
   ],
   "source": [
    "# Doing the heavyweight initialization of deeppavlov library here\n",
    "from deeppavlov import build_model, configs\n",
    "model = build_model(configs.squad.squad_bert, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a watch'], [562], [1523999.75]]\n",
      "[['you say \" What a pity!\"?\\' the Rabbit asked.'], [6], [0.03372638300061226]]\n",
      "[[''], [-1], [2.575139045715332]]\n",
      "[['You did'], [2], [6.767302513122559]]\n",
      "[[''], [-1], [0.09891779720783234]]\n"
     ]
    }
   ],
   "source": [
    "predicted_answer_list = []\n",
    "for passage in top_passages:\n",
    "    predicted_answer = model([passage], [question])\n",
    "    predicted_answer_list.append(predicted_answer)\n",
    "    print(predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2> Question: 'What did the rabbit took out of its waistcoat-pocket?'</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<h2> Question: '\" + question + \"'</h2>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2> Best Answer: 'a watch'</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h2> Best Answer: ' + str(\"'\" + predicted_answer_list[0][0][0] + \"'\") + '</h2>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b : Use HuggingFace API to extract the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Representation and Tokenization In BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"[CLS] \" + question + \" [SEP] \" + top_passages[0] + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(input_text)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself ,' Oh dear! Oh dear! I shall be late!' ( when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural ); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT - POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat - pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit - hole under the hedge.\n"
     ]
    }
   ],
   "source": [
    "print(top_passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'did', 'the', 'rabbit', 'took', 'out', 'of', 'its', 'waist', '##coat', '-', 'pocket', '?', '[SEP]', 'there', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'rabbit', 'say', 'to', 'itself', ',', \"'\", 'oh', 'dear', '!', 'oh', 'dear', '!', 'i', 'shall', 'be', 'late', '!', \"'\", '(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ')', ';', 'but', 'when', 'the', 'rabbit', 'actually', 'took', 'a', 'watch', 'out', 'of', 'its', 'waist', '##coat', '-', 'pocket', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waist', '##coat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = tokenizer.encode(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('what', 2054)\n",
      "('did', 2106)\n",
      "('the', 1996)\n",
      "('rabbit', 10442)\n",
      "('took', 2165)\n",
      "('out', 2041)\n",
      "('of', 1997)\n",
      "('its', 2049)\n",
      "('waist', 5808)\n",
      "('##coat', 16531)\n",
      "('-', 1011)\n",
      "('pocket', 4979)\n",
      "('?', 1029)\n",
      "('[SEP]', 102)\n",
      "('there', 2045)\n",
      "('was', 2001)\n",
      "('nothing', 2498)\n",
      "('so', 2061)\n",
      "('very', 2200)\n",
      "('remarkable', 9487)\n",
      "('in', 1999)\n",
      "('that', 2008)\n",
      "(';', 1025)\n",
      "('nor', 4496)\n",
      "('did', 2106)\n",
      "('alice', 5650)\n",
      "('think', 2228)\n",
      "('it', 2009)\n",
      "('so', 2061)\n",
      "('very', 2200)\n",
      "('much', 2172)\n",
      "('out', 2041)\n",
      "('of', 1997)\n",
      "('the', 1996)\n",
      "('way', 2126)\n",
      "('to', 2000)\n",
      "('hear', 2963)\n",
      "('the', 1996)\n",
      "('rabbit', 10442)\n",
      "('say', 2360)\n",
      "('to', 2000)\n",
      "('itself', 2993)\n",
      "(',', 1010)\n",
      "(\"'\", 1005)\n",
      "('oh', 2821)\n",
      "('dear', 6203)\n",
      "('!', 999)\n",
      "('oh', 2821)\n",
      "('dear', 6203)\n",
      "('!', 999)\n",
      "('i', 1045)\n",
      "('shall', 4618)\n",
      "('be', 2022)\n",
      "('late', 2397)\n",
      "('!', 999)\n",
      "(\"'\", 1005)\n",
      "('(', 1006)\n",
      "('when', 2043)\n",
      "('she', 2016)\n",
      "('thought', 2245)\n",
      "('it', 2009)\n",
      "('over', 2058)\n",
      "('afterwards', 5728)\n",
      "(',', 1010)\n",
      "('it', 2009)\n",
      "('occurred', 4158)\n",
      "('to', 2000)\n",
      "('her', 2014)\n",
      "('that', 2008)\n",
      "('she', 2016)\n",
      "('ought', 11276)\n",
      "('to', 2000)\n",
      "('have', 2031)\n",
      "('wondered', 4999)\n",
      "('at', 2012)\n",
      "('this', 2023)\n",
      "(',', 1010)\n",
      "('but', 2021)\n",
      "('at', 2012)\n",
      "('the', 1996)\n",
      "('time', 2051)\n",
      "('it', 2009)\n",
      "('all', 2035)\n",
      "('seemed', 2790)\n",
      "('quite', 3243)\n",
      "('natural', 3019)\n",
      "(')', 1007)\n",
      "(';', 1025)\n",
      "('but', 2021)\n",
      "('when', 2043)\n",
      "('the', 1996)\n",
      "('rabbit', 10442)\n",
      "('actually', 2941)\n",
      "('took', 2165)\n",
      "('a', 1037)\n",
      "('watch', 3422)\n",
      "('out', 2041)\n",
      "('of', 1997)\n",
      "('its', 2049)\n",
      "('waist', 5808)\n",
      "('##coat', 16531)\n",
      "('-', 1011)\n",
      "('pocket', 4979)\n",
      "(',', 1010)\n",
      "('and', 1998)\n",
      "('looked', 2246)\n",
      "('at', 2012)\n",
      "('it', 2009)\n",
      "(',', 1010)\n",
      "('and', 1998)\n",
      "('then', 2059)\n",
      "('hurried', 9520)\n",
      "('on', 2006)\n",
      "(',', 1010)\n",
      "('alice', 5650)\n",
      "('started', 2318)\n",
      "('to', 2000)\n",
      "('her', 2014)\n",
      "('feet', 2519)\n",
      "(',', 1010)\n",
      "('for', 2005)\n",
      "('it', 2009)\n",
      "('flashed', 8373)\n",
      "('across', 2408)\n",
      "('her', 2014)\n",
      "('mind', 2568)\n",
      "('that', 2008)\n",
      "('she', 2016)\n",
      "('had', 2018)\n",
      "('never', 2196)\n",
      "('before', 2077)\n",
      "('seen', 2464)\n",
      "('a', 1037)\n",
      "('rabbit', 10442)\n",
      "('with', 2007)\n",
      "('either', 2593)\n",
      "('a', 1037)\n",
      "('waist', 5808)\n",
      "('##coat', 16531)\n",
      "('-', 1011)\n",
      "('pocket', 4979)\n",
      "(',', 1010)\n",
      "('or', 2030)\n",
      "('a', 1037)\n",
      "('watch', 3422)\n",
      "('to', 2000)\n",
      "('take', 2202)\n",
      "('out', 2041)\n",
      "('of', 1997)\n",
      "('it', 2009)\n",
      "(',', 1010)\n",
      "('and', 1998)\n",
      "('burning', 5255)\n",
      "('with', 2007)\n",
      "('curiosity', 10628)\n",
      "(',', 1010)\n",
      "('she', 2016)\n",
      "('ran', 2743)\n",
      "('across', 2408)\n",
      "('the', 1996)\n",
      "('field', 2492)\n",
      "('after', 2044)\n",
      "('it', 2009)\n",
      "(',', 1010)\n",
      "('and', 1998)\n",
      "('fortunately', 14599)\n",
      "('was', 2001)\n",
      "('just', 2074)\n",
      "('in', 1999)\n",
      "('time', 2051)\n",
      "('to', 2000)\n",
      "('see', 2156)\n",
      "('it', 2009)\n",
      "('pop', 3769)\n",
      "('down', 2091)\n",
      "('a', 1037)\n",
      "('large', 2312)\n",
      "('rabbit', 10442)\n",
      "('-', 1011)\n",
      "('hole', 4920)\n",
      "('under', 2104)\n",
      "('the', 1996)\n",
      "('hedge', 17834)\n",
      "('.', 1012)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "for tup in zip(tokenized_text, input_ids):\n",
    "  print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6253, -6.5432, -8.7005, -8.0701, -7.9872, -7.1824, -8.0202, -7.7935,\n",
       "         -8.3308, -8.4675, -9.2599, -9.4378, -8.8390, -9.9604, -5.6253, -7.8081,\n",
       "         -8.1429, -6.5101, -7.7626, -8.4542, -7.7951, -7.0669, -6.8516, -8.6917,\n",
       "         -7.7679, -7.9444, -5.3592, -7.5644, -7.2754, -7.7703, -8.5000, -8.1266,\n",
       "         -7.1917, -8.1409, -8.4497, -8.4238, -7.9103, -8.4523, -7.7646, -5.8052,\n",
       "         -8.8358, -8.5498, -7.9195, -8.7839, -6.5267, -5.7339, -8.1382, -8.4639,\n",
       "         -6.5532, -7.8100, -8.3539, -5.7651, -7.6919, -8.0590, -6.0737, -8.7583,\n",
       "         -8.2796, -8.1480, -7.4105, -7.7558, -7.7056, -7.7925, -8.5667, -7.6174,\n",
       "         -8.4638, -7.4137, -7.4711, -8.6020, -7.8537, -8.5254, -7.4074, -8.2212,\n",
       "         -8.6080, -8.7363, -7.5475, -8.9741, -7.7709, -8.7558, -7.8884, -6.8745,\n",
       "         -7.1873, -7.9396, -6.4894, -7.6782, -6.4856, -6.7629, -5.9999, -8.1147,\n",
       "         -7.3338, -5.7741, -2.6644,  0.3923, -1.2396, -2.3462,  0.3755,  5.8996,\n",
       "          4.7149, -4.0624, -4.6867, -3.3212, -2.5265, -6.0798, -6.3112, -3.3221,\n",
       "         -6.2991, -5.6115, -3.6131, -6.9317, -4.2585, -6.6629, -6.5927, -5.3633,\n",
       "         -4.2255, -5.7031, -5.7508, -2.4723, -6.2641, -7.6915, -7.2933, -6.0594,\n",
       "         -7.3643, -6.3740, -5.8813, -6.7580, -7.7059, -7.4251, -7.1075, -6.6209,\n",
       "         -5.3019, -6.9576, -4.5970, -7.2357, -7.2584, -5.6622, -3.8983, -5.3478,\n",
       "         -6.1391, -1.1727, -3.6279, -7.8189, -7.7495, -5.4828, -8.1055, -6.6251,\n",
       "          3.3274,  2.8594, -5.5987, -5.8162, -6.7969, -7.2150, -6.2806, -7.6853,\n",
       "         -7.6769, -5.4133, -8.2433, -6.3675, -8.3924, -5.7479, -5.9943, -7.5180,\n",
       "         -8.2745, -7.3354, -7.1505, -7.2978, -8.1564, -7.5582, -6.2453, -7.1502,\n",
       "         -7.0156, -7.6335, -7.6315, -7.7035, -7.3478, -6.2644, -5.7150, -7.1810,\n",
       "         -6.8277, -6.2969, -5.3224, -8.7361, -7.4705, -7.8103, -8.5722, -7.0056,\n",
       "         -8.1889, -5.6253]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores = response['start_logits']\n",
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6253, -6.5432, -8.7005, -8.0701, -7.9872, -7.1824, -8.0202, -7.7935,\n",
       "         -8.3308, -8.4675, -9.2599, -9.4378, -8.8390, -9.9604, -5.6253, -7.8081,\n",
       "         -8.1429, -6.5101, -7.7626, -8.4542, -7.7951, -7.0669, -6.8516, -8.6917,\n",
       "         -7.7679, -7.9444, -5.3592, -7.5644, -7.2754, -7.7703, -8.5000, -8.1266,\n",
       "         -7.1917, -8.1409, -8.4497, -8.4238, -7.9103, -8.4523, -7.7646, -5.8052,\n",
       "         -8.8358, -8.5498, -7.9195, -8.7839, -6.5267, -5.7339, -8.1382, -8.4639,\n",
       "         -6.5532, -7.8100, -8.3539, -5.7651, -7.6919, -8.0590, -6.0737, -8.7583,\n",
       "         -8.2796, -8.1480, -7.4105, -7.7558, -7.7056, -7.7925, -8.5667, -7.6174,\n",
       "         -8.4638, -7.4137, -7.4711, -8.6020, -7.8537, -8.5254, -7.4074, -8.2212,\n",
       "         -8.6080, -8.7363, -7.5475, -8.9741, -7.7709, -8.7558, -7.8884, -6.8745,\n",
       "         -7.1873, -7.9396, -6.4894, -7.6782, -6.4856, -6.7629, -5.9999, -8.1147,\n",
       "         -7.3338, -5.7741, -2.6644,  0.3923, -1.2396, -2.3462,  0.3755,  5.8996,\n",
       "          4.7149, -4.0624, -4.6867, -3.3212, -2.5265, -6.0798, -6.3112, -3.3221,\n",
       "         -6.2991, -5.6115, -3.6131, -6.9317, -4.2585, -6.6629, -6.5927, -5.3633,\n",
       "         -4.2255, -5.7031, -5.7508, -2.4723, -6.2641, -7.6915, -7.2933, -6.0594,\n",
       "         -7.3643, -6.3740, -5.8813, -6.7580, -7.7059, -7.4251, -7.1075, -6.6209,\n",
       "         -5.3019, -6.9576, -4.5970, -7.2357, -7.2584, -5.6622, -3.8983, -5.3478,\n",
       "         -6.1391, -1.1727, -3.6279, -7.8189, -7.7495, -5.4828, -8.1055, -6.6251,\n",
       "          3.3274,  2.8594, -5.5987, -5.8162, -6.7969, -7.2150, -6.2806, -7.6853,\n",
       "         -7.6769, -5.4133, -8.2433, -6.3675, -8.3924, -5.7479, -5.9943, -7.5180,\n",
       "         -8.2745, -7.3354, -7.1505, -7.2978, -8.1564, -7.5582, -6.2453, -7.1502,\n",
       "         -7.0156, -7.6335, -7.6315, -7.7035, -7.3478, -6.2644, -5.7150, -7.1810,\n",
       "         -6.8277, -6.2969, -5.3224, -8.7361, -7.4705, -7.8103, -8.5722, -7.0056,\n",
       "         -8.1889, -5.6253]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_scores = response['end_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'did', 'the', 'rabbit', 'took', 'out', 'of', 'its', 'waist', '##coat', '-', 'pocket', '?', '[SEP]', 'there', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'rabbit', 'say', 'to', 'itself', ',', \"'\", 'oh', 'dear', '!', 'oh', 'dear', '!', 'i', 'shall', 'be', 'late', '!', \"'\", '(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ')', ';', 'but', 'when', 'the', 'rabbit', 'actually', 'took', 'a', 'watch', 'out', 'of', 'its', 'waist', '##coat', '-', 'pocket', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waist', '##coat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a watch'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.369904518127441"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(end_scores).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting answers for from all the top passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a watch\n",
      "what a pity ! \" ? ' the rabbit asked .\n",
      "drinking\n",
      "what did the rabbit took out of its waist ##coat - pocket ? [SEP] ' you did ! ' said the hat ##ter\n",
      "what did the rabbit took out of its waist ##coat - pocket\n"
     ]
    }
   ],
   "source": [
    "predicted_answer_list = []\n",
    "for passage in top_passages:\n",
    "    input_text = \"[CLS] \" + question + \" [SEP] \" + passage + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "    response = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "    start_scores = response['start_logits']\n",
    "    end_scores = response['end_logits']\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    predicted_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "    predicted_answer_list.append(predicted_answer)\n",
    "    print(predicted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"background-color: #99CD4E; text-align:center; vertical-align: middle; padding:40px 0;\"> \n",
    "  <h1 style=\"color: white;\"> The End </h1>.\n",
    " </div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
